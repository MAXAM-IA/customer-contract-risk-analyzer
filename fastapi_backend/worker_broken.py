import time
import json
import pandas as pd
from pathlib import Path
import logging
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import base64
import httpx
import os

# Cargar variables de entorno desde .env si existe
try:
    from dotenv import load_dotenv
    load_dotenv()
    logger = logging.getLogger(__name__)
    logger.info("‚úÖ Variables de entorno cargadas desde .env")
except ImportError:
    logger = logging.getLogger(__name__)
    logger.info("‚ö†Ô∏è  python-dotenv no instalado, usando variables de entorno del sistema")

# Configurar logging para worker
logger = logging.getLogger(__name__)

def analizar_pregunta(pregunta, seccion, pdf_bytes, pdf_filename="documento.pdf"):
    """
    Analiza una pregunta usando Gemini LLM y adjunta el PDF (en bytes) como contexto multimodal.
    Devuelve un dict con 'Respuesta' y 'Riesgo'.
    """
    logger.info(f"üß† ANALIZANDO PREGUNTA: '{pregunta[:50]}...' | Secci√≥n: {seccion} | Archivo: {pdf_filename}")
    
    try:
        api_key = os.getenv("GOOGLE_API_KEY", "")
        if not api_key:
            logger.error("‚ùå GOOGLE_API_KEY no est√° configurada")
            return {
                "Respuesta": "Error: GOOGLE_API_KEY no configurada",
                "Riesgo": "Alto"
            }
        
        logger.info(f"üîë API Key configurada correctamente")
        
        LLM_MODEL = "gemini-2.5-flash-preview-05-20"
        logger.info(f"ü§ñ Inicializando modelo: {LLM_MODEL}")
        
        llm = ChatGoogleGenerativeAI(
            model=LLM_MODEL,
            temperature=0,
            max_tokens=None,
            max_retries=2,
            http_client=httpx.Client(verify=False),
            google_api_key=api_key
        )
        logger.info(f"‚úÖ Modelo inicializado correctamente")

        logger.info(f"üìÑ Convirtiendo PDF a base64 (tama√±o: {len(pdf_bytes)} bytes)")
        base64_bytes = base64.b64encode(pdf_bytes)
        base64_string = base64_bytes.decode("utf-8")
        logger.info(f"‚úÖ PDF convertido a base64 (tama√±o: {len(base64_string)} caracteres)")

        prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente legal experto en an√°lisis de contratos. Responde de forma clara y precisa a la pregunta del usuario, usando el documento adjunto como contexto. 

Al final de tu respuesta, eval√∫a el nivel de riesgo legal bas√°ndote en los siguientes criterios:
- ALTO: Cl√°usulas que pueden generar responsabilidades significativas, terminaci√≥n unilateral, penalizaciones severas, t√©rminos ambiguos que favorecen a la contraparte, ausencia de protecciones importantes.
- MEDIO: T√©rminos que requieren atenci√≥n pero no representan riesgos inmediatos, cl√°usulas est√°ndar con posibles mejoras.
- BAJO: T√©rminos favorables o neutros, cl√°usulas est√°ndar de la industria, protecciones adecuadas.

Termina tu respuesta con una l√≠nea que indique claramente: "RIESGO: [ALTO/MEDIO/BAJO]" """),
            ("human", f"Secci√≥n: {seccion}\nPregunta: {pregunta}"),
            (
                "human",
                [
                    {
                        "type": "file",
                        "source_type": "base64",
                        "filename": pdf_filename,
                        "data": base64_string,
                        "mime_type": "application/pdf",
                    }
                ],
            )
        ])
        
        logger.info(f"üìù Enviando consulta al LLM...")
        chain = prompt | llm | StrOutputParser()
        respuesta_llm = chain.invoke({})
        logger.info(f"‚úÖ Respuesta recibida del LLM (longitud: {len(respuesta_llm)} caracteres)")
        
        # Extraer el nivel de riesgo de la respuesta del LLM
        riesgo = "Medio"  # Valor por defecto
        if "RIESGO:" in respuesta_llm:
            lineas = respuesta_llm.split('\n')
            for linea in lineas:
                if "RIESGO:" in linea.upper():
                    if "ALTO" in linea.upper():
                        riesgo = "Alto"
                    elif "BAJO" in linea.upper():
                        riesgo = "Bajo"
                    elif "MEDIO" in linea.upper():
                        riesgo = "Medio"
                    break
        
        logger.info(f"üéØ Riesgo evaluado: {riesgo}")
        
        # Limpiar la respuesta removiendo la l√≠nea de riesgo para que no aparezca duplicada
        respuesta_limpia = respuesta_llm
        if "RIESGO:" in respuesta_llm:
            lineas = respuesta_llm.split('\n')
            lineas_filtradas = [linea for linea in lineas if not linea.upper().startswith("RIESGO:")]
            respuesta_limpia = '\n'.join(lineas_filtradas).strip()
        
        logger.info(f"‚úÖ An√°lisis completado exitosamente")
        return {
            "Respuesta": respuesta_limpia,
            "Riesgo": riesgo
        }
        
    except Exception as e:
        logger.error(f"‚ùå ERROR al analizar pregunta: {str(e)}", exc_info=True)
        return {
            "Respuesta": f"Error al analizar la pregunta: {str(e)}",
            "Riesgo": "Alto"
        }

def pdf_file_to_base64_string(filepath):
    with open(filepath, "rb") as f:
        pdf_bytes = f.read()
        base64_bytes = base64.b64encode(pdf_bytes)
        base64_string = base64_bytes.decode("utf-8")
        return base64_string

def analizar_pregunta_llm(pregunta, seccion, pdf_path):
    """
    Analiza una pregunta usando Gemini LLM y adjunta el PDF como contexto.
    Devuelve un dict con 'Respuesta' y 'Riesgo' (demo: riesgo bajo si no se menciona 'terminate').
    """
    api_key = os.getenv("GOOGLE_API_KEY", "")
    LLM_MODEL = "gemini-2.5-flash-preview-05-20"
    llm = ChatGoogleGenerativeAI(
        model=LLM_MODEL,
        temperature=0,
        max_tokens=None,
        max_retries=2,
        http_client=httpx.Client(verify=False),
        google_api_key=api_key
    )

    pdf_data = pdf_file_to_base64_string(pdf_path)
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Eres un asistente legal experto en an√°lisis de contratos. Responde de forma clara y precisa a la pregunta del usuario, usando el documento adjunto como contexto. Si la pregunta implica un riesgo legal relevante, ind√≠calo."),
        ("human", f"Secci√≥n: {seccion}\nPregunta: {pregunta}"),
        (
            "human",
            [
                {
                    "type": "file",
                    "source_type": "base64",
                    "filename": os.path.basename(pdf_path),
                    "data": pdf_data,
                    "mime_type": "application/pdf",
                }
            ],
        )
    ])
    chain = prompt | llm | StrOutputParser()
    respuesta_llm = chain.invoke({})
    riesgo = "Alto" if "terminate" in pregunta.lower() else "Bajo"
    return {
        "Respuesta": respuesta_llm,
        "Riesgo": riesgo
    }

def analizar_documento(contrato_path, preguntas_path, progreso_path):
    logger.info(f"üöÄ INICIANDO AN√ÅLISIS DE DOCUMENTO EN SEGUNDO PLANO")
    logger.info(f"üìÑ Contrato: {contrato_path}")
    logger.info(f"‚ùì Preguntas: {preguntas_path}")
    logger.info(f"üìä Progreso: {progreso_path}")
    
    contrato_path = Path(contrato_path)
    
    # Actualizar estado inmediatamente a "iniciando"
    try:
        with open(progreso_path, "w", encoding="utf-8") as f:
            json.dump({
                "estado": "iniciando", 
                "resultados": [], 
                "mensaje": "Iniciando an√°lisis del documento..."
            }, f, ensure_ascii=False, indent=2)
        logger.info(f"ÔøΩ Estado inicial guardado: 'iniciando'")
    except Exception as e:
        logger.error(f"‚ùå Error guardando estado inicial: {str(e)}")
    
    try:
        logger.info(f"ÔøΩüìñ Leyendo archivo de contrato...")
        # Leer el archivo seg√∫n su extensi√≥n
        if contrato_path.suffix.lower() == '.pdf':
            logger.info(f"üîç Archivo detectado como PDF")
            with open(contrato_path, "rb") as f:
                pdf_bytes = f.read()
            usar_pdf = True
            logger.info(f"‚úÖ PDF le√≠do exitosamente ({len(pdf_bytes)} bytes)")
        else:
            logger.info(f"üîç Archivo detectado como texto ({contrato_path.suffix})")
            # Para archivos de texto, leer como texto
            with open(contrato_path, "r", encoding="utf-8", errors="replace") as f:
                texto_contrato = f.read()
            usar_pdf = False
            logger.info(f"‚úÖ Texto le√≠do exitosamente ({len(texto_contrato)} caracteres)")
    except Exception as e:
        logger.error(f"‚ùå ERROR leyendo contrato: {str(e)}")
        with open(progreso_path, "w", encoding="utf-8") as f:
            json.dump({"estado": "error", "resultados": [], "error": f"Error leyendo contrato: {str(e)}"}, f, ensure_ascii=False, indent=2)
        return

    # Actualizar estado a "leyendo preguntas"
    try:
        with open(progreso_path, "w", encoding="utf-8") as f:
            json.dump({
                "estado": "en_progreso", 
                "resultados": [], 
                "mensaje": "Leyendo archivo de preguntas..."
            }, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"‚ùå Error actualizando estado: {str(e)}")

    try:
        logger.info(f"üìã Leyendo archivo de preguntas...")
        preguntas_df = pd.read_excel(preguntas_path)
        logger.info(f"‚úÖ Archivo de preguntas le√≠do exitosamente ({len(preguntas_df)} preguntas)")
    except Exception as e:
        logger.error(f"‚ùå ERROR leyendo preguntas: {str(e)}")
        with open(progreso_path, "w", encoding="utf-8") as f:
            json.dump({"estado": "error", "resultados": [], "error": f"Error leyendo preguntas: {str(e)}"}, f, ensure_ascii=False, indent=2)
        return

    resultados = []
    preguntas_originales = []
    
    logger.info(f"üîÑ Procesando preguntas iniciales...")
    for idx, row in preguntas_df.iterrows():
        numero = row.get("N√∫mero de Pregunta") or row.get("Numero de Pregunta") or row.get("N¬∫ de Pregunta") or ""
        pregunta = row.get("Pregunta") or ""
        seccion = row.get("Secci√≥n") or row.get("Seccion") or ""
        preguntas_originales.append({
            "N√∫mero": numero,
            "Pregunta": pregunta,
            "Secci√≥n": seccion
        })
    
    logger.info(f"‚úÖ {len(preguntas_originales)} preguntas procesadas")
    
    # Guardar estado inicial con preguntas originales
    logger.info(f"üíæ Guardando estado inicial con preguntas...")
    with open(progreso_path, "w", encoding="utf-8") as f:
        json.dump({
            "estado": "en_progreso", 
            "preguntas_originales": preguntas_originales, 
            "resultados": resultados,
            "mensaje": f"Preparando an√°lisis de {len(preguntas_originales)} preguntas..."
        }, f, ensure_ascii=False, indent=2)

    logger.info(f"üéØ COMENZANDO AN√ÅLISIS DE PREGUNTAS...")
    for idx, row in preguntas_df.iterrows():
        numero = row.get("N√∫mero de Pregunta") or row.get("Numero de Pregunta") or row.get("N¬∫ de Pregunta") or ""
        pregunta = row.get("Pregunta") or ""
        seccion = row.get("Secci√≥n") or row.get("Seccion") or ""
        
        logger.info(f"üîç PREGUNTA {idx+1}/{len(preguntas_df)} - {numero}: '{pregunta[:50]}...'")
        
        # Actualizar estado con pregunta actual
        try:
            with open(progreso_path, "w", encoding="utf-8") as f:
                json.dump({
                    "estado": "en_progreso", 
                    "preguntas_originales": preguntas_originales, 
                    "resultados": resultados,
                    "mensaje": f"Analizando pregunta {idx+1}/{len(preguntas_df)}: {pregunta[:30]}..."
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå Error actualizando progreso pregunta {idx+1}: {str(e)}")
        
        # Usar la funci√≥n apropiada seg√∫n el tipo de archivo
        try:
            if usar_pdf:
                logger.info(f"üìÑ Analizando con funci√≥n PDF...")
                salida = analizar_pregunta(pregunta, seccion, pdf_bytes, contrato_path.name)
            else:
                logger.info(f"üìù Analizando con funci√≥n texto...")
                salida = analizar_pregunta_texto(pregunta, seccion, texto_contrato)
            
            logger.info(f"‚úÖ Pregunta {idx+1} completada - Riesgo: {salida['Riesgo']}")
        except Exception as e:
            logger.error(f"‚ùå ERROR en pregunta {idx+1}: {str(e)}")
            salida = {
                "Respuesta": f"Error al procesar la pregunta: {str(e)}",
                "Riesgo": "Alto"
            }
            
        resultados.append({
            "N√∫mero": numero,
            "Pregunta": pregunta,
            "Secci√≥n": seccion,
            "Estado": "‚úÖ Completado",
            "Respuesta": salida["Respuesta"],
            "Riesgo": salida["Riesgo"]
        })
        
        # Actualizar progreso despu√©s de cada pregunta
        logger.info(f"üíæ Actualizando progreso ({idx+1}/{len(preguntas_df)})...")
        try:
            with open(progreso_path, "w", encoding="utf-8") as f:
                json.dump({
                    "estado": "en_progreso", 
                    "preguntas_originales": preguntas_originales, 
                    "resultados": resultados,
                    "mensaje": f"Completadas {len(resultados)}/{len(preguntas_originales)} preguntas"
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå Error guardando progreso: {str(e)}")

    logger.info(f"üéâ AN√ÅLISIS COMPLETADO - {len(resultados)} preguntas procesadas")
    try:
        with open(progreso_path, "w", encoding="utf-8") as f:
            json.dump({
                "estado": "completado", 
                "preguntas_originales": preguntas_originales, 
                "resultados": resultados,
                "mensaje": f"An√°lisis completado exitosamente - {len(resultados)} preguntas procesadas"
            }, f, ensure_ascii=False, indent=2)
        logger.info(f"‚úÖ Estado final guardado en {progreso_path}")
    except Exception as e:
        logger.error(f"‚ùå Error guardando estado final: {str(e)}")

def analizar_documento_con_preguntas_custom(contrato_path, preguntas_custom, progreso_path):
    """
    Analiza un documento con preguntas personalizadas (para rean√°lisis global).
    """
    contrato_path = Path(contrato_path)
    
    try:
        # Leer el archivo seg√∫n su extensi√≥n
        if contrato_path.suffix.lower() == '.pdf':
            with open(contrato_path, "rb") as f:
                pdf_bytes = f.read()
            usar_pdf = True
        else:
            # Para archivos de texto, leer como texto
            with open(contrato_path, "r", encoding="utf-8", errors="replace") as f:
                texto_contrato = f.read()
            usar_pdf = False
    except Exception as e:
        with open(progreso_path, "w", encoding="utf-8") as f:
            json.dump({"estado": "error", "resultados": [], "error": f"Error leyendo contrato: {str(e)}"}, f, ensure_ascii=False, indent=2)
        return

    resultados = []
    preguntas_originales = []
    
    # Usar las preguntas personalizadas
    for pregunta_data in preguntas_custom:
        preguntas_originales.append({
            "N√∫mero": pregunta_data.get("numero", ""),
            "Pregunta": pregunta_data.get("pregunta", ""),
            "Secci√≥n": pregunta_data.get("seccion", "")
        })
    
    # Guardar estado inicial con preguntas originales
    with open(progreso_path, "w", encoding="utf-8") as f:
        json.dump({"estado": "en_progreso", "preguntas_originales": preguntas_originales, "resultados": resultados}, f, ensure_ascii=False, indent=2)

    for idx, pregunta_data in enumerate(preguntas_custom):
        numero = pregunta_data.get("numero", "")
        pregunta = pregunta_data.get("pregunta", "")
        seccion = pregunta_data.get("seccion", "")
        
        # Usar la funci√≥n apropiada seg√∫n el tipo de archivo
        if usar_pdf:
            salida = analizar_pregunta(pregunta, seccion, pdf_bytes, contrato_path.name)
        else:
            salida = analizar_pregunta_texto(pregunta, seccion, texto_contrato)
            
        resultados.append({
            "N√∫mero": numero,
            "Pregunta": pregunta,
            "Secci√≥n": seccion,
            "Estado": "‚úÖ Completado",
            "Respuesta": salida["Respuesta"],
            "Riesgo": salida["Riesgo"]
        })
        
        # Actualizar progreso despu√©s de cada pregunta
        with open(progreso_path, "w", encoding="utf-8") as f:
            json.dump({"estado": "en_progreso", "preguntas_originales": preguntas_originales, "resultados": resultados}, f, ensure_ascii=False, indent=2)

    # Marcar como completado
    with open(progreso_path, "w", encoding="utf-8") as f:
        json.dump({"estado": "completado", "preguntas_originales": preguntas_originales, "resultados": resultados}, f, ensure_ascii=False, indent=2)

def analizar_pregunta_texto(pregunta, seccion, texto_contrato):
    """
    Analiza una pregunta usando Gemini LLM con texto plano como contexto.
    Usado para archivos que no son PDF.
    Devuelve un dict con 'Respuesta' y 'Riesgo'.
    """
    logger.info(f"üìù ANALIZANDO PREGUNTA CON TEXTO: '{pregunta[:50]}...' | Secci√≥n: {seccion}")
    
    try:
        api_key = os.getenv("GOOGLE_API_KEY", "")
        if not api_key:
            logger.error("‚ùå GOOGLE_API_KEY no est√° configurada")
            return {
                "Respuesta": "Error: GOOGLE_API_KEY no configurada",
                "Riesgo": "Alto"
            }
        
        logger.info(f"üîë API Key configurada correctamente")
        
        LLM_MODEL = "gemini-2.5-flash-preview-05-20"
        logger.info(f"ü§ñ Inicializando modelo: {LLM_MODEL}")
        
        llm = ChatGoogleGenerativeAI(
            model=LLM_MODEL,
            temperature=0,
            max_tokens=None,
            max_retries=2,
            http_client=httpx.Client(verify=False),
            google_api_key=api_key
        )
        logger.info(f"‚úÖ Modelo inicializado correctamente")

        logger.info(f"üìÑ Preparando texto para an√°lisis (longitud: {len(texto_contrato)} caracteres)")
        prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente legal experto en an√°lisis de contratos. Responde de forma clara y precisa a la pregunta del usuario, usando el documento adjunto como contexto. 

Al final de tu respuesta, eval√∫a el nivel de riesgo legal bas√°ndote en los siguientes criterios:
- ALTO: Cl√°usulas que pueden generar responsabilidades significativas, terminaci√≥n unilateral, penalizaciones severas, t√©rminos ambiguos que favorecen a la contraparte, ausencia de protecciones importantes.
- MEDIO: T√©rminos que requieren atenci√≥n pero no representan riesgos inmediatos, cl√°usulas est√°ndar con posibles mejoras.
- BAJO: T√©rminos favorables o neutros, cl√°usulas est√°ndar de la industria, protecciones adecuadas.

Termina tu respuesta con una l√≠nea que indique claramente: "RIESGO: [ALTO/MEDIO/BAJO]" """),
            ("human", f"Secci√≥n: {seccion}\nPregunta: {pregunta}\n\nDocumento a analizar:\n{texto_contrato}")
        ])
        
        logger.info(f"üìù Enviando consulta al LLM...")
        chain = prompt | llm | StrOutputParser()
        respuesta_llm = chain.invoke({})
        logger.info(f"‚úÖ Respuesta recibida del LLM (longitud: {len(respuesta_llm)} caracteres)")
        
        # Extraer el nivel de riesgo de la respuesta del LLM
        riesgo = "Medio"  # Valor por defecto
        if "RIESGO:" in respuesta_llm:
            lineas = respuesta_llm.split('\n')
            for linea in lineas:
                if "RIESGO:" in linea.upper():
                    if "ALTO" in linea.upper():
                        riesgo = "Alto"
                    elif "BAJO" in linea.upper():
                        riesgo = "Bajo"
                    elif "MEDIO" in linea.upper():
                        riesgo = "Medio"
                    break
        
        logger.info(f"üéØ Riesgo evaluado: {riesgo}")
        
        # Limpiar la respuesta removiendo la l√≠nea de riesgo para que no aparezca duplicada
        respuesta_limpia = respuesta_llm
